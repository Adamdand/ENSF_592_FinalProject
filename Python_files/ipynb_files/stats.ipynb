{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats.ipynb (computing grid data file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 10x10 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_boundary_layer = pd.read_csv('..\\..\\CSV_files\\City_Boundary_Layer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df_city_boundary_layer[\"the_geom\"].values[0]\n",
    "string_stripped= string.replace(\"POLYGON\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\")\n",
    "string_split = string_stripped.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_long = []\n",
    "list_lat = []\n",
    "\n",
    "#appends latitudes and longitudes based on long/lat/long/lat... pattern\n",
    "for i in range(len(string_split)):\n",
    "    if i % 2 == 0:\n",
    "        list_long.append(float(string_split[i]))\n",
    "    else:\n",
    "        list_lat.append(float(string_split[i]))\n",
    "        \n",
    "min_long = min(list_long)\n",
    "max_long = max(list_long)\n",
    "min_lat = min(list_lat)\n",
    "max_lat = max(list_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_box = box(min_long,min_lat,max_long,max_lat)\n",
    "geo = gpd.GeoSeries([coord_box]).__geo_interface__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[min_lat,min_long], zoom_start=10)\n",
    "folium.GeoJson(geo).add_to(map_osm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ten_split = np.linspace(min_long,max_long,num = 11)\n",
    "lat_ten_split = np.linspace(min_lat,max_lat,num = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_array = []\n",
    "\n",
    "for y in range(len(lat_ten_split)-1):\n",
    "    for x in range(len(long_ten_split)-1):\n",
    "        bot_left = [long_ten_split[x],lat_ten_split[y]]\n",
    "        bot_right = [long_ten_split[x+1],lat_ten_split[y]]\n",
    "        top_left = [long_ten_split[x],lat_ten_split[y+1]]\n",
    "        top_right = [long_ten_split[x+1],lat_ten_split[y+1]]\n",
    "        grid_array.append([bot_left,bot_right,top_left,top_right])\n",
    "\n",
    "        coord_box = box(bot_left[0],bot_left[1],top_right[0],top_right[1])\n",
    "        geo = gpd.GeoSeries([coord_box]).__geo_interface__\n",
    "        folium.GeoJson(geo).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations for each 1x1 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method deals with csv files containing multistring longitude/latitude\n",
    "\n",
    "# Iterate through each line of the csv file. Parse the multistring from the csv file to extract the values below for each row and store each value in the corresponding list:\n",
    "# minimum longitude\n",
    "# minimum latitude\n",
    "# maximum longitude\n",
    "# maximum latitude\n",
    "\n",
    "# Check if a coordinate is within each 1x1 grid.\n",
    "\n",
    "# Return a list of matching speed limit or volume for each 1x1 grid, depending on the parameters passed.\n",
    "\n",
    "\n",
    "def compute_average_with_multistring(fileName, columnName, stringName, columnName2, grid_array):\n",
    "    \n",
    "    df = pd.read_csv(fileName)\n",
    "\n",
    "    list_long = []\n",
    "    list_lat = []\n",
    "\n",
    "    min_long = []\n",
    "    max_long = []\n",
    "    min_lat = []\n",
    "    max_lat = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # parsing \"multiline\" column to extract longitude/latitude\n",
    "        message1 = df[columnName].values[index]\n",
    "        message1 = message1.replace(stringName, '').replace('(', '').replace(',', '').replace(')', '')\n",
    "        message1 = message1.split()\n",
    "        \n",
    "        # the multiline string follows longitude, latitude, longitude, latitude...... pattern\n",
    "        for i in range(len(message1)):\n",
    "            if i % 2 == 0:\n",
    "                list_long.append(float(message1[i]))\n",
    "            else:\n",
    "                list_lat.append(float(message1[i]))\n",
    "\n",
    "        # finding min/max longitude/latitude for each row    \n",
    "        min_long.append(min(list_long))\n",
    "        max_long.append(max(list_long))\n",
    "        min_lat.append(min(list_lat))\n",
    "        max_lat.append(max(list_lat))\n",
    "\n",
    "    # consist of 100 lists corresponding to the 100 grids\n",
    "    # each individual list stores the extracted speed limit if the coordinate is within the grid\n",
    "    resultList = [[] for _ in range(100)]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for i in range(len(grid_array)):\n",
    "            \n",
    "            # To reduce search time:\n",
    "            # 1. Check if (minimum longitude, minimum latitude) is greater than the top right coordinate of the 1x1 grid.\n",
    "            # 2. Check if (maximum longitude, maximum latitude) is smaller than the bottom left coordinate of the 1x1 grid.\n",
    "            # If the above condition meets, none of the coordinates from the current row of the multiline column would lie within the current 1x1 grid.\n",
    "\n",
    "            # For each 1x1 grid:\n",
    "            # grid_array[i][3][0] = longitude of top right coordinate\n",
    "            # grid_array[i][3][1] = latitude of top right coordinate\n",
    "            # grid_array[i][0][0] = longitude of bottom left coordinate\n",
    "            # grid_array[i][0][1] = latitude of bottom left coordinate\n",
    "\n",
    "            if((min_long[index] > grid_array[i][3][0] and min_lat[index] > grid_array[i][3][1]) or (max_long[index] < grid_array[i][0][0] and max_lat[index] < grid_array[i][0][1])):\n",
    "                break\n",
    "            \n",
    "            # If the above condition does not meet, there is the possibility of having coordinates lie within the current 1x1 grid. We would need to continue our search.\n",
    "            else:\n",
    "                # parsing \"multiline\" column to extract longitude/latitude\n",
    "                message1 = df[columnName].values[index]\n",
    "                message1 = message1.replace(stringName, '').replace('(', '').replace(',', '').replace(')', '')\n",
    "                message1 = message1.split()\n",
    "\n",
    "                # converting to float type\n",
    "                for x in range(len(message1)):\n",
    "                    message1[x] = float(message1[x])\n",
    "                \n",
    "                # If one of the coordinate from the multistring meets the condition, we break outside the loop and check the next row.\n",
    "                # This means that we only consider each road once within each grid.\n",
    "                # For example, if a road appears 5 times within 1 grid, we only count it once.\n",
    "                for j in range(0, len(message1), 2):\n",
    "                    if((grid_array[i][0][0] <= message1[j]) and (message1[j] <= grid_array[i][3][0]) and (grid_array[i][0][1] <= message1[j+1]) and (message1[j+1] <= grid_array[i][3][1])):\n",
    "                        resultList[i].append(df[columnName2].values[index])\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method deals with csv files containing only 1 value for longitude/latitude\n",
    "\n",
    "# Iterate through each line of the csv file.\n",
    "\n",
    "# Check if a coordinate is within each 1x1 grid.\n",
    "\n",
    "# Return a list of results for each 1x1 grid.\n",
    "\n",
    "def compute_result(df, columnLong, columnLat, grid_array):\n",
    "\n",
    "    resultList = [[] for _ in range(100)]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for i in range(len(grid_array)):\n",
    "            if((grid_array[i][0][0] <= df[columnLong][index]) and (df[columnLong][index] <= grid_array[i][3][0]) and (grid_array[i][0][1] <= df[columnLat][index]) and (df[columnLat][index] <= grid_array[i][3][1])):\n",
    "                resultList[i].append(1)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing average speed limit for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df_average_speed_limit = pd.DataFrame(compute_average_with_multistring('..\\..\\CSV_files\\Speed_Limits.csv', 'multiline', 'MULTILINESTRING', 'SPEED', grid_array))\n",
    "# compute average\n",
    "df_average_speed_limit['Average Speed Limit'] = df_average_speed_limit.mean(axis=1)\n",
    "df_average_speed_limit = df_average_speed_limit[['Average Speed Limit']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing average traffic volume for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "df_average_traffic_volumes = pd.DataFrame(compute_average_with_multistring('..\\..\\CSV_files\\Traffic_Volumes_for_2018.csv', 'multilinestring', 'MULTILINESTRING', 'VOLUME', grid_array))\n",
    "# compute average\n",
    "df_average_traffic_volumes['Average Traffic Volume'] = df_average_traffic_volumes.mean(axis=1)\n",
    "df_average_traffic_volumes = df_average_traffic_volumes[['Average Traffic Volume']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing traffic cameras for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_cameras = pd.read_csv('..\\..\\CSV_files\\Traffic_Camera_Locations.csv')\n",
    "# convert to dataframe\n",
    "df_traffic_cameras = pd.DataFrame(compute_result(df_traffic_cameras, 'longitude', 'latitude', grid_array))\n",
    "# compute sum\n",
    "df_traffic_cameras['Traffic Cameras'] = df_traffic_cameras.sum(axis=1)\n",
    "df_traffic_cameras = df_traffic_cameras[['Traffic Cameras']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing traffic signals for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_signals = pd.read_csv('..\\..\\CSV_files\\Traffic_Signals.csv')\n",
    "# convert to dataframe\n",
    "df_traffic_signals = pd.DataFrame(compute_result(df_traffic_signals, 'longitude', 'latitude', grid_array))\n",
    "# compute sum\n",
    "df_traffic_signals['Traffic Signals'] = df_traffic_signals.sum(axis=1)\n",
    "df_traffic_signals = df_traffic_signals[['Traffic Signals']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing traffic signs for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_signs = pd.read_csv('..\\..\\CSV_files\\Traffic_Signs.csv')\n",
    "\n",
    "# extracting relevant signs\n",
    "df_traffic_signs = df_traffic_signs.loc[(df_traffic_signs['BLADE_TYPE'] == 'Regulatory') | (df_traffic_signs['BLADE_TYPE'] == 'Warning') | (df_traffic_signs['BLADE_TYPE'] == 'Stop') | (df_traffic_signs['BLADE_TYPE'] == 'Playground') | (df_traffic_signs['BLADE_TYPE'] == 'Yield') | (df_traffic_signs['BLADE_TYPE'] == 'Pedestrian') | (df_traffic_signs['BLADE_TYPE'] == 'Disabled Parking') | (df_traffic_signs['BLADE_TYPE'] == 'Speed') | (df_traffic_signs['BLADE_TYPE'] == 'Bicycle / Pathway') | (df_traffic_signs['BLADE_TYPE'] == 'School')]\n",
    "\n",
    "# reset index\n",
    "df_traffic_signs = df_traffic_signs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs_per_grid = [[] for _ in range(100)]\n",
    "\n",
    "for index, row in df_traffic_signs.iterrows():\n",
    "    for i in range(len(grid_array)):\n",
    "            message1 = df_traffic_signs['POINT'].values[index]\n",
    "            message1 = message1.replace('POINT', '').replace('(', '').replace(')', '')\n",
    "            message1 = message1.split()\n",
    "            \n",
    "            for x in range(2):\n",
    "                message1[x] = float(message1[x])\n",
    "\n",
    "            if((grid_array[i][0][0] <= message1[0]) and (message1[0] <= grid_array[i][3][0]) and (grid_array[i][0][1] <= message1[1]) and (message1[1] <= grid_array[i][3][1])):\n",
    "                signs_per_grid[i].append(1)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "df_traffic_signs = pd.DataFrame(signs_per_grid)\n",
    "# convert to dataframe\n",
    "df_traffic_signs['Traffic Signs'] = df_traffic_signs.sum(axis=1)\n",
    "# compute sum\n",
    "df_traffic_signs = df_traffic_signs[['Traffic Signs']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing traffic incidents for each area/grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_incidents = pd.read_csv('..\\..\\CSV_files\\Traffic_Incidents.csv')\n",
    "# extract 2018 incidents\n",
    "df_traffic_incidents = df_traffic_incidents[df_traffic_incidents['id'].str.startswith(str(2018))]\n",
    "\n",
    "# convert to dataframe\n",
    "df_traffic_incidents = pd.DataFrame(compute_result(df_traffic_incidents, 'Longitude', 'Latitude', grid_array))\n",
    "# compute sum\n",
    "df_traffic_incidents['Traffic Incidents'] = df_traffic_incidents.sum(axis=1)\n",
    "df_traffic_incidents = df_traffic_incidents[['Traffic Incidents']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining results into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index column corresponds to the 100 grids:\n",
    "# from bottom left (grid 0) to top right (grid 99)\n",
    "df = pd.concat([df_average_speed_limit, df_average_traffic_volumes, df_traffic_cameras, df_traffic_signals, df_traffic_signs, df_traffic_incidents], axis=1).reindex(df_average_speed_limit.index)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}